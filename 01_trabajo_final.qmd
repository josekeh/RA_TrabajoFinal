---
title: "Trabajo Práctico Final"
format: pdf
lang: es 
editor: visual
editor_options: 
  chunk_output_type: console
execute: 
  echo: false
  warning: false
---

# Introducción

El básquetbol es uno de los deportes más populares en los Estados Unidos. Además de su destacadísima liga masculina NBA, se encuentra su similar femenina, la altamente competitiva, WNBA. El siguiente trabajo busca demostrar conocimientos parendidos en la materia Regresión avanzada a partir de un dataset de esta ultima liga mencionada.

# Dataset

## Librerías

```{r}
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(broom)
library(egg)
library(ggpubr)
library(car)
library(glmnet)
```

## Importación de datos

```{r}
data <- read.csv("files/WNBA.csv", sep = ";")
summary(data)
```

```{r}
data$Equipo <- as.factor(data$Equipo)
data$Posicion <- as.factor(data$Posicion)
summary(data)
```

## Breve análisis

### Vacíos y duplicados

A través del summary se puede ver que las variables numéricas están correctamente definidas, sin NAs. Los mismo para Equipo y Posición. Se procede a analizar si existen jugadoras campo Jugadora vacío

```{r}
paste0("Hay ",count(data %>% filter(Jugadora == "")), " registros vacíos")
```

Una vez checkeado esto, viendo que no hay vacíos, vemos si hay alguna jugadora repetida.

```{r}
data[duplicated(data$Jugadora),]
```

Vemos que existen jugadoras que aparecen más de una vez, pero parece ser que se encuentran en equipos diferentes.

```{r}
print(count(data[duplicated(data[, c("Jugadora", "Equipo")]), ]))
```

Por ende, podemos entender que cada vez que aparece una jugadora se da en otro club. Esto también puede llevar a diferentes condiciones de goleo como así también otro momento de la carrera profesional, por lo que se decide conservar los datos tal cual se encuentran.

### Entendiendo el dataset

Una vez avanzada la etapa anterior, vamos a ver brevemente cómo se encuentran distribuidos los datos.

```{r}
ggplot(data, aes(x = Equipo)) +
  geom_bar(fill = "grey", color = "black") +
  labs(title = "Equipos",
       x = "Equipo",
       y = "Frecuencia") +
  theme_minimal()
```

```{r}
ggplot(data, aes(x = Posicion)) +
  geom_bar(fill = "grey", color = "black") +
  labs(title = "Equipos",
       x = "Posición",
       y = "Frecuencia") +
  theme_minimal()
```

Se puede observar como es similar la distribución de jugadoras por equipo, mientras que abundan mayormente posiciones de Escolta, seguido de Alero y luego Pivot. El tener pocos datos de Pivots no es algo irracional, ya que es es la posición que menos jugadoras suele haber.

```{r}
skimr::skim(data %>% select(where(is.numeric)))
```

-   ToDo

# Modelos de Regresión Lineal

Para comenzar a trabajar en los modelos de regresión lineal, se procede a dividir el dataset en train/test. Se define una semilla para hacer reproducible el experimento y se define, por consigna, que el train contenga el 70% de los datos, mientras que lo restante queda para test.

```{r}
# División en training y testing
set.seed(45672848)
index_training <- sample(1:nrow(data), size = 0.7*nrow(data))

training <- data[index_training,]
testing <- data[-index_training,]
```

Se procede, luego, a obtener tres modelos mediante los siguientes criterios: - Usando stepwise regression - Criterios propios - Preguntando al famoso ChatGPT

## Stepwise regression

Para el primer caso se busca obtener un modelo de regresión en el que las variables a utilizar se definan por el proceso automático de Stepwise regression. Para el análisis se quitan del dataset el nombre de la jugadora y el equipo, ya que dichas variables cuentan con una gran cantidad de categorías. En caso de incluirlas, se obtendrían estimaciones muy pobres debido a los escasos grados de libertad, o incluso podría llegar el caso que sea imposible estimar el modelo.

```{r}
training_filtered <- select(training, -Jugadora, -Equipo)
m1 <- MASS::stepAIC(
  object = lm(Puntos ~ 1, data = training_filtered), #punto de partida
  scope = list(upper = lm(Puntos ~ ., data = training_filtered)), #máximo modelo posible
  direction = "both", #método de selección
  trace = FALSE, #para no imprimir resultados parciales
  k = 2, #penalización a emplear (2 = AIC, log(n) = BIC)
  steps = 1000 #máximo nro de pasos
  )

summary(m1)
```

El resultado devuelve la utilización de Dobles, Triples, Robos y Juegos.

## Criterios propios

Observando el dataset, se plantea un modelo donde los puntos se basen en dobles y triples, y debido a que debería de haber una diferencia (por los libres), se plantea la posición, la cual determinaría la tendencia de un jugador a recibir faltas de tiro.

```{r}
m2 <- lm(Puntos ~ Dobles + Triples + Posicion, data = training_filtered)

summary(m2)
```

## ChatGPT

Para este tercer modelo, se le ha preguntado (o desafiado) a ChatGPT a recomendar cuál sería el mejor modelo. La respuesta fue incluir Dobles, Triples, Minutos, Asistencias, Rebotes y Posicion.

```{r}
m3 <- lm(Puntos ~ Triples + Dobles + Minutos + Asistencias + Rebotes + Posicion, data = training)
```

## Comparación de modelos

Se procede a comparar los modelos planteados comparando CME, PRESS, Cp, AIC y BIC.

```{r}
modelos <- data.frame(Modelo = c("Modelo 1", "Modelo 2", "Modelo 3"),
                      Ajuste = c("m1", "m2", "m3"))
```

```{r}
Modelo_completo <- lm(Puntos ~ . -Jugadora - Equipo, data = training)
sigma <- sum(residuals(Modelo_completo)^2)/df.residual(Modelo_completo)

Resumen <- modelos %>% 
  group_by(Modelo) %>% 
  summarise(CME = summary(get(Ajuste))$sigma^2,
            PRESS = sum((resid(get(Ajuste))/(1-hatvalues(get(Ajuste))))^2),
            Cp = sum(residuals(get(Ajuste))^2)/sigma + 2 * length(coef(get(Ajuste))) - nrow(training),
            AIC = AIC(get(Ajuste)),
            BIC = BIC(get(Ajuste)))

kable(Resumen, caption = "Tabla N°1: Comparación del ajuste de los 3 modelos") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

Con los datos obtenidos se puede pensar en elegir entre el Modelo 1 y Modelo 2. El Modelo 1 es el que mejores resultados en las métricas de performance, mientras que el Modelo 2 es apenas peor, pero es más simple. Para este caso, se decide quedarse con el Modelo 1.

## Análisis de residuos

Ya con el Modelo 1 elegido, se procede a evaluar los supuestos a través de los residuos.

```{r}
Modelo_final <- augment(m1)
g1 <- ggplot(mapping = aes(sample = Modelo_final$.resid)) + 
    stat_qq(color = "dodgerblue") + 
    stat_qq_line() + 
  ggtitle("QQ Normal") +
  xlab("Cuantiles teóricos") +
  ylab("Cuantiles muestrales") +
  geom_text(aes(x = 2, y = -150, fill ="black", label = paste0("p-value \n", round(shapiro.test(Modelo_final$.resid)$p.value, 8)))) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = "none")

g2 <- ggplot(Modelo_final) +
  aes(x = .fitted, y = .std.resid) +
  geom_point(color = "dodgerblue") +
  geom_hline(yintercept = 0, color = "black") +
  ggtitle("Residuos vs. Predichos") +
  xlab("Valores predichos") +
  ylab("Residuos estandarizados") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

g <- ggarrange(g1, g2, ncol = 2)

annotate_figure(g, top = text_grob("Gráfico N°1: Comprobación de supuestos", face = "bold", size = 14))
```

En el gráfico N°1 se observa que la distribución de los mismos presenta colas pesadas y que no se verifica el supuesto de variancia constante. En caso de querer solucionarlo, podría optarse por utilizar una transformación de la variables respuesta como el $log(Y)$ o bien aplicar el método de Box y Cox.

Luego se procede a analizar colinealidad

```{r}
# Colinealidad
training_modelo <- training %>% 
  select(Juegos, Minutos, Triples, Dobles)

corr_matrix <- cor(training_modelo)

corr_melted <- reshape2::melt(corr_matrix)

ggplot(corr_melted, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  geom_text(aes(label = round(value, 2)), size = 3) +
  scale_fill_gradient2(low = "#ef2947", high = "#009929", mid = "white", midpoint = 0) +
  labs(x = "", y = "", title = "Gráfico N°2: Matriz de correlación") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```

Analizando las correlaciones lineales entre variables se observa que minutos está muy relacionada linealmente con todas las restantes.

```{r}
vif = vif(m1)

vif_df <- data.frame(Variable = names(vif),
                     VIF = as.numeric(vif),
                     row.names = NULL)

kable(vif_df, caption = "Tabla N°2: Factores de Inflación de la Varianza (VIF)") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F)
```

Al analizar la tabla N°2, se ve que, a pesar de que las variables estén altamente correlacionadas, esto no tiene un impacto tan importante ya que ningún VIF es mayor a 5.

```{r}
# Observaciones atípicas
Modelo_final <- mutate(Modelo_final, 
                       PRESS = .resid/(1-.hat),
                       id = 1:nrow(Modelo_final))

g1 <- ggplot(data = Modelo_final) + 
    aes(x = id, y = PRESS) + 
  geom_bar(stat="identity", color = "dodgerblue", fill = "dodgerblue")+
    geom_hline(aes(yintercept = 0)) +
    ggtitle("Residuos PRESS") +
    xlab("Observación") +
    ylab("PRESS") +
    theme_bw() +
    theme(legend.position = "none",
          plot.title = element_text(hjust = 0.5))

# Observaciones influyentes
g2 <- ggplot(data = Modelo_final) +
    aes(x = id, y = .cooksd) +
  geom_bar(stat="identity", color = "dodgerblue", fill = "dodgerblue")+
    ggtitle("Distancia de Cook") +
    xlab("Observación") +
    ylab("Distancia de Cook") +
    theme_bw() +
    theme(legend.position = "none",
      plot.title = element_text(hjust = 0.5))

g <- ggarrange(g1, g2, ncol = 2)

annotate_figure(g, top = text_grob("Gráfico N°3: Observaciones atípicas e influyentes", face = "bold", size = 14))
```

En el gráfico N°3 se vuelve a ver que existe una observación atípica, la cual presenta un residuo PRESS mayor a 100. Sin embargo, al analizar la influencia de las observaciones mediante la distancia de Cook, se encuentra que ninguna tiene una influencia desmedida sobre el ajuste del modelo, ya que todas son menores a 1.

## Interpretación del modelo

```{r}
Betas = data.frame(Variable = names(coef(m1)),
                   Valor = as.numeric(coef(m1)),
                   P_value = as.numeric(coef(summary(m1))[, "Pr(>|t|)"]))

kable(Betas, caption = "Tabla N°3: Coeficientes estimados del modelo") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F)

as.numeric(coef(summary(m2))[, "Pr(>|t|)"])
```

Todo: - Redactar intro con analisis descriptivo breve (chino) - Redactar punto 1 (chino) - Riedge y Lasso + redacción (Fede)

# Métodos de Regularización

Una vez identificado el mejor modelo de regresión con el ajuste de mínimos cuadrados ordinarios, se optó por evaluar la posibilidad de utilizar métodos de regularización. Esto se hizo con el objetivo de intentar mejorar la capacidad predictiva, basándonos en la idea del *trade-off* entre sesgo y variancia.

Para realizar tanto el ajuste mediante la regresión ridge, como por el método de lasso, primero se buscó el mejor valor de $\lambda$ utilizando *5-fold Cross-Validation.*

Los resultados de los ajustes junto con los valores de $\lambda$ utilizados, son presentados a continuación:

```{r}
# Preparamos la matriz de predictores
training_X <- training %>% 
  select(Dobles, Triples, Robos, Juegos) %>% 
  as.matrix()

# Buscamos el lambda óptimo para ridge y lasso
lambda_ridge <- cv.glmnet(training_X, training$Puntos, nfolds = 5, alpha = 0)$lambda.min 

lambda_lasso <- cv.glmnet(training_X, training$Puntos, nfolds = 5, alpha = 1)$lambda.min 

# Ajustamos la regresión ridge y la regresión lasso
m_ridge <- glmnet(x = training_X, y = training$Puntos, alpha = 0, lambda = lambda_ridge)

m_lasso <- glmnet(x = training_X, y = training$Puntos, alpha = 1, lambda = lambda_ridge)
coefficients(m_lasso)
```

```{r}
modelos <- data.frame(Modelo = c(paste0("Modelo Ridge (lambda = ", round(lambda_ridge, 3), ")"), paste0("Modelo Lasso (lambda = ", round(lambda_lasso, 3), ")"), "Modelo MCO"),
                      Ajuste = c("m_ridge", "m_lasso", "m1"))

modelos$Modelo <- factor(modelos$Modelo, levels = c("Modelo Ridge (lambda = 19.131)", "Modelo Lasso (lambda = 0.496)", "Modelo MCO"))

Comparacion <- modelos %>% 
  group_by(Modelo) %>% 
  summarise(Intercept = round(coefficients(get(Ajuste))[1], 2),
            Dobles = round(coefficients(get(Ajuste))[2], 2),
            Triples = round(coefficients(get(Ajuste))[3], 2),
            Robos = round(coefficients(get(Ajuste))[4], 2),
            Juegos = round(coefficients(get(Ajuste))[5], 2))

kable(Comparacion, caption = "Tabla N°4: Comparación de los coeficientes estimados por los 3 modelos") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

Al analizar la Tabla N°4, puede verse que los valores óptimos de $\lambda$ varían mucho según se trata de la regresión Ridge o del método de Lasso. Respecto a los coeficientes estimados vemos que, como indica la teoría, en general han reducido su valor. En el caso de la regresión Ridge destaca que se invirtieron los signos del intercepto y la variable juegos. En el caso de Lasso, destaca que el coeficiente de juegos lo forzó a ser igual a 0 y eso trajo aparejado un aumento importante en el valor del intercepto.

## Capacidad predictiva

Una vez presentado el ajuste de los 3 modelos, se evaluó su poder predictivo en base al error cuadrático medio en el conjunto de testeo.

```{r}
testing_X <- testing %>% 
  select(Dobles, Triples, Robos, Juegos) %>% 
  as.matrix()

pred_ridge <- predict(m_ridge, testing_X)
pred_lasso <- predict(m_lasso, testing_X)
pred_MCO <- predict(m1, testing)

MSE_ridge <- mean((testing$Puntos-pred_ridge)^2)
MSE_lasso <- mean((testing$Puntos-pred_lasso)^2)
MSE_MCO <- mean((testing$Puntos-pred_MCO)^2)

modelos$MSE = c(MSE_ridge, MSE_lasso, MSE_MCO)

kable(modelos[,c(1,3)], caption = "Tabla N°5: Comparación de la capacidad predictiva de los 3 modelos") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

Sorprendentemente, la Tabla N°5 muestra que el mejor ajuste fue el de mínimos cuadrados ordinarios, el cual presenta un error 60% menor al de los modelos de Ridge y Lasso. Esto nos indica que, a pesar de que no se cumplan los supuestos y hayamos identificado posibles problemas de colinealidad, a la hora de predecir mínimo cuadrados fue más performante que el resto.
