---
title: "Trabajo Práctico Final"
format: pdf
lang: es 
editor: visual
editor_options: 
  chunk_output_type: console
execute: 
  echo: false
  warning: false
  error: true
---

# Introducción

El básquetbol es uno de los deportes más populares en los Estados Unidos. Además de su destacadísima liga masculina NBA, se encuentra su similar femenina, la altamente competitiva, WNBA. El siguiente trabajo busca demostrar conocimientos aprendidos en la materia Regresión avanzada a partir de un dataset de esta ultima liga mencionada.

# Dataset

## Librerías

```{r}
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(broom)
library(egg)
library(ggpubr)
library(car)
library(glmnet)
library(caret)
library(pROC)
```

## Importación de datos

```{r}
data <- read.csv("files/WNBA.csv", sep = ";")
summary(data)
```

```{r}
data$Equipo <- as.factor(data$Equipo)
data$Posicion <- as.factor(data$Posicion)
summary(data)
```

## Breve análisis

### Vacíos y duplicados

A través del summary se puede ver que las variables numéricas están correctamente definidas, sin NAs. Los mismo para Equipo y Posición. Se procede a analizar si existen jugadoras campo Jugadora vacío

```{r}
paste0("Hay ",count(data %>% filter(Jugadora == "")), " registros vacíos")
```

Una vez checkeado esto, viendo que no hay vacíos, vemos si hay alguna jugadora repetida.

```{r}
data[duplicated(data$Jugadora),]
```

Vemos que existen jugadoras que aparecen más de una vez, pero parece ser que se encuentran en equipos diferentes.

```{r}
print(count(data[duplicated(data[, c("Jugadora", "Equipo")]), ]))
```

Por ende, podemos entender que cada vez que aparece una jugadora se da en otro club. Esto también puede llevar a diferentes condiciones de goleo como así también otro momento de la carrera profesional, por lo que se decide conservar los datos tal cual se encuentran.

### Entendiendo el dataset

Una vez avanzada la etapa anterior, vamos a ver brevemente cómo se encuentran distribuidos los datos.

```{r}
ggplot(data, aes(x = Equipo)) +
  geom_bar(fill = "grey", color = "black") +
  labs(title = "Equipos",
       x = "Equipo",
       y = "Frecuencia") +
  theme_minimal()
```

```{r}
ggplot(data, aes(x = Posicion)) +
  geom_bar(fill = "grey", color = "black") +
  labs(title = "Equipos",
       x = "Posición",
       y = "Frecuencia") +
  theme_minimal()
```

Se puede observar como es similar la distribución de jugadoras por equipo, mientras que abundan mayormente posiciones de Escolta, seguido de Alero y luego Pivot. El tener pocos datos de Pivots no es algo irracional, ya que es es la posición que menos jugadoras suele haber.

```{r}
skimr::skim(data %>% select(where(is.numeric)))
```

Analizando las variables cuantitativas vemos en primer lugar que no hay valores negativos, lo cual es coherente y requerido. Mientras que la mayoría de las distribuciones son asimétricas hacia la derecha, se puede observar como la cantidad de juegos es la única que no respeta este patrón. Resulta apropiado ver cómo el histograma de dobles y triples es muy parecido en la forma de la distribución como así también el resultante natural de los puntos.

También es interesante comparar medias de Triples y Dobles con Puntos. Si se realizar rápidamente Triples x 3 + Dobles x 2 (en medias), se obtiene un valor de 186.7, lejano al 226 de los Puntos, por lo que podríamos entender, a priori, que los libres (tiros individuales de 1 punto) tiene peso en el goleo de las jugadoras.

# Modelos de Regresión Lineal

Para comenzar a trabajar en los modelos de regresión lineal, se procede a dividir el dataset en train/test. Se define una semilla para hacer reproducible el experimento y se define, por consigna, que el train contenga el 70% de los datos, mientras que lo restante queda para test.

```{r}
# División en training y testing
set.seed(45672848)
index_training <- sample(1:nrow(data), size = 0.7*nrow(data))

training <- data[index_training,]
testing <- data[-index_training,]
```

Se procede, luego, a obtener tres modelos mediante los siguientes criterios: - Usando stepwise regression - Criterios propios - Preguntando al famoso ChatGPT

## Stepwise regression

Para el primer caso se busca obtener un modelo de regresión en el que las variables a utilizar se definan por el proceso automático de Stepwise regression, ya que utilizar la técnica del mejor subconjunto llevaría a un número muy grande de modelos a comprar y se opta por esta técnica que si bien no garantiza encontrar el óptimo, puede darnos una buena primera aproximación. Para el análisis se quitan del dataset el nombre de la jugadora y el equipo, ya que dichas variables cuentan con una gran cantidad de categorías. En caso de incluirlas, se obtendrían estimaciones muy pobres debido a los escasos grados de libertad, o incluso podría llegar el caso que sea imposible estimar el modelo.

```{r}
training_filtered <- select(training, -Jugadora, -Equipo)
m1 <- MASS::stepAIC(
  object = lm(Puntos ~ 1, data = training_filtered), #punto de partida
  scope = list(upper = lm(Puntos ~ ., data = training_filtered)), #máximo modelo posible
  direction = "both", #método de selección
  trace = FALSE, #para no imprimir resultados parciales
  k = 2, #penalización a emplear (2 = AIC, log(n) = BIC)
  steps = 1000 #máximo nro de pasos
  )

summary(m1)
```

El resultado devuelve la utilización de Dobles, Triples, Robos y Juegos.

## Criterios propios

Observando el dataset, se plantea un modelo donde los puntos se basen en dobles y triples, y debido a que debería de haber una diferencia (por los libres), se plantea la posición, la cual determinaría la tendencia de un jugador a recibir faltas de tiro.

```{r}
m2 <- lm(Puntos ~ Dobles + Triples + Posicion, data = training_filtered)

summary(m2)
```

## ChatGPT

Para este tercer modelo, se le ha preguntado (o desafiado) a ChatGPT a recomendar cuál sería el mejor modelo. La respuesta fue incluir Dobles, Triples, Minutos, Asistencias, Rebotes y Posicion.

```{r}
m3 <- lm(Puntos ~ Triples + Dobles + Minutos + Asistencias + Rebotes + Posicion, data = training)
```

## Comparación de modelos

Se procede a comparar los modelos planteados comparando CME, PRESS, Cp, AIC y BIC.

```{r}
modelos <- data.frame(Modelo = c("Modelo 1", "Modelo 2", "Modelo 3"),
                      Ajuste = c("m1", "m2", "m3"))
```

```{r}
Modelo_completo <- lm(Puntos ~ . -Jugadora - Equipo, data = training)
sigma <- sum(residuals(Modelo_completo)^2)/df.residual(Modelo_completo)

Resumen <- modelos %>% 
  group_by(Modelo) %>% 
  summarise(CME = summary(get(Ajuste))$sigma^2,
            PRESS = sum((resid(get(Ajuste))/(1-hatvalues(get(Ajuste))))^2),
            Cp = sum(residuals(get(Ajuste))^2)/sigma + 2 * length(coef(get(Ajuste))) - nrow(training),
            AIC = AIC(get(Ajuste)),
            BIC = BIC(get(Ajuste)))

kable(Resumen, caption = "Tabla N°1: Comparación del ajuste de los 3 modelos") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

Con los datos obtenidos se puede pensar en elegir entre el Modelo 1 y Modelo 2. El Modelo 1 es el que mejores resultados en las métricas de performance, mientras que el Modelo 2 es apenas peor, pero es más simple. Para este caso, se decide quedarse con el Modelo 1.

## Análisis de residuos

Ya con el Modelo 1 elegido, se procede a evaluar los supuestos a través de los residuos.

```{r}
Modelo_final <- augment(m1)
g1 <- ggplot(mapping = aes(sample = Modelo_final$.resid)) + 
    stat_qq(color = "dodgerblue") + 
    stat_qq_line() + 
  ggtitle("QQ Normal") +
  xlab("Cuantiles teóricos") +
  ylab("Cuantiles muestrales") +
  geom_text(aes(x = 2, y = -50, fill ="black", label = paste0("p-value \n", round(shapiro.test(Modelo_final$.resid)$p.value, 8)))) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = "none")

g2 <- ggplot(Modelo_final) +
  aes(x = .fitted, y = .std.resid) +
  geom_point(color = "dodgerblue") +
  geom_hline(yintercept = 0, color = "black") +
  ggtitle("Residuos vs. Predichos") +
  xlab("Valores predichos") +
  ylab("Residuos estandarizados") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

g <- ggarrange(g1, g2, ncol = 2)

annotate_figure(g, top = text_grob("Gráfico N°1: Comprobación de supuestos", face = "bold", size = 14))
```

En el gráfico N°1 se observa que la distribución de los mismos presenta colas pesadas y que no se verifica el supuesto de variancia constante. En caso de querer solucionarlo, podría optarse por utilizar una transformación de la variables respuesta como el $log(Y)$ o bien aplicar el método de Box y Cox. Por otro lado, también se observa a un posible valor atípico.

Luego se procede a analizar colinealidad

```{r}
# Colinealidad
training_modelo <- training %>% 
  select(Juegos, Minutos, Triples, Dobles)

corr_matrix <- cor(training_modelo)

corr_melted <- reshape2::melt(corr_matrix)

ggplot(corr_melted, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  geom_text(aes(label = round(value, 2)), size = 3) +
  scale_fill_gradient2(low = "#ef2947", high = "#009929", mid = "white", midpoint = 0) +
  labs(x = "", y = "", title = "Gráfico N°2: Matriz de correlación") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```

Analizando las correlaciones lineales entre variables se observa que minutos está muy relacionada linealmente con todas las restantes.

```{r}
vif = vif(m1)

vif_df <- data.frame(Variable = names(vif),
                     VIF = as.numeric(vif),
                     row.names = NULL)

kable(vif_df, caption = "Tabla N°2: Factores de Inflación de la Varianza (VIF)") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F)
```

Al analizar la tabla N°2, se ve que, a pesar de que las variables estén altamente correlacionadas, esto no tiene un impacto tan importante ya que ningún VIF es mayor a 5.

```{r}
# Observaciones atípicas
Modelo_final <- mutate(Modelo_final, 
                       PRESS = .resid/(1-.hat),
                       id = 1:nrow(Modelo_final))

g1 <- ggplot(data = Modelo_final) + 
    aes(x = id, y = PRESS) + 
  geom_bar(stat="identity", color = "dodgerblue", fill = "dodgerblue")+
    geom_hline(aes(yintercept = 0)) +
    ggtitle("Residuos PRESS") +
    xlab("Observación") +
    ylab("PRESS") +
    theme_bw() +
    theme(legend.position = "none",
          plot.title = element_text(hjust = 0.5))

# Observaciones influyentes
g2 <- ggplot(data = Modelo_final) +
    aes(x = id, y = .cooksd) +
  geom_bar(stat="identity", color = "dodgerblue", fill = "dodgerblue")+
    ggtitle("Distancia de Cook") +
    xlab("Observación") +
    ylab("Distancia de Cook") +
    theme_bw() +
    theme(legend.position = "none",
      plot.title = element_text(hjust = 0.5))

g <- ggarrange(g1, g2, ncol = 2)

annotate_figure(g, top = text_grob("Gráfico N°3: Observaciones atípicas e influyentes", face = "bold", size = 14))
```

En el gráfico N°3 se vuelve a ver que existe una observación atípica, la cual presenta un residuo PRESS mayor a 100. Sin embargo, al analizar la influencia de las observaciones mediante la distancia de Cook, se encuentra que ninguna tiene una influencia desmedida sobre el ajuste del modelo, ya que todas son menores a 1.

## Interpretación del modelo

```{r}
Betas = data.frame(Variable = names(coef(m1)),
                   Valor = as.numeric(coef(m1)),
                   P_value = as.numeric(coef(summary(m1))[, "Pr(>|t|)"]))

kable(Betas, caption = "Tabla N°3: Coeficientes estimados del modelo") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F)
```

A través de la Tabla N°3 se puede interpretar nuestro modelo elegido:

-   El valor estimado del intercepto es de 2.68 aproximadamente. Igual así, el pvalue resulta de no poder descartar la hipótesis de que el intercepto es igual a 0. Recordando la interpretación del mismo, se pensaría que es el valor de puntos promedio para 0 dobles, 0 triples, 0 robos y 0 juegos. Lo cual no resulta para nada ilógico pensar que este valor sea 0 en la realidad y que estadísticamente nos de dicho indicio.

Los demás predictores tienen un pvalue \<0.01, algunos con ordenes mucho menores, por lo que podemos rechazar la hipotesis de nula. Respecto a sus valores predichos podemos decir que:

-   Por cada doble convertido, los puntos de una jugadora aumentan en promedio en una suma de 2.40, manteniendose todas las demás variables constantes.
-   Por cada triple convertido, los puntos de una jugadora aumentan en promedio en una suma de 3.46, manteniendose todas las demás variables constantes.
-   Por cada robo conseguido, los puntos de una jugadora aumentan en promedio en una suma de 0.74, manteniendose todas las demás variables constantes.
-   Por cada juego jugado, los puntos de una jugadora disminuyen en promedio en una suma de 0.51, manteniendose todas las demás variables constantes.

Estas interpretaciones nos dejan insights interesantes de analizar. Si bien dobles y triples suman 2 y 3 puntos respectivamente en el juego, que el valor que afectan al promedio sea mayor puede estar indicando que en cierto modo, estos coeficientes están absorbiendo el efecto de los tiros libres. Lo mismo podemos pensar acerca del robo, ya que cuando se da el mismo, en el desorden del juego, es probable que quien roba reciba una falta. Por último, el valor al aumentar la cantidad de juegos podría estar dando indicio de desgaste físico de las jugadoras, o bien podría ser que, a pesar de haber encontrado que la multicolinealidad no era muy importante, en realidad si nos está afectando a la estimación de este coeficiente, que a priori uno esperaría que fuera positivo.

# Métodos de Regularización

Una vez identificado el mejor modelo de regresión con el ajuste de mínimos cuadrados ordinarios, se optó por evaluar la posibilidad de utilizar métodos de regularización. Esto se hizo con el objetivo de intentar mejorar la capacidad predictiva, basándose en la idea del *trade-off* entre sesgo y variancia.

Para realizar tanto el ajuste mediante la regresión ridge, como por el método de lasso, primero se buscó el mejor valor de $\lambda$ utilizando *5-fold Cross-Validation.*

Los resultados de los ajustes junto con los valores de $\lambda$ utilizados, son presentados a continuación:

```{r}
# Preparamos la matriz de predictores
training_X <- training %>% 
  select(Dobles, Triples, Robos, Juegos) %>% 
  as.matrix()

# Buscamos el lambda óptimo para ridge y lasso
lambda_ridge <- cv.glmnet(training_X, training$Puntos, nfolds = 5, alpha = 0)$lambda.min 

lambda_lasso <- cv.glmnet(training_X, training$Puntos, nfolds = 5, alpha = 1)$lambda.min 

# Ajustamos la regresión ridge y la regresión lasso
m_ridge <- glmnet(x = training_X, y = training$Puntos, alpha = 0, lambda = lambda_ridge)

m_lasso <- glmnet(x = training_X, y = training$Puntos, alpha = 1, lambda = lambda_ridge)
coefficients(m_lasso)
```

```{r}
modelos <- data.frame(Modelo = c("Modelo Ridge", "Modelo Lasso", "Modelo MCO"),
                      Lambda = c(round(lambda_ridge, 3), round(lambda_lasso, 3), "-"),
                      Ajuste = c("m_ridge", "m_lasso", "m1"))

modelos$Modelo <- factor(modelos$Modelo, levels = c("Modelo Ridge", "Modelo Lasso", "Modelo MCO"))

Comparacion <- modelos %>% 
  group_by(Modelo, Lambda) %>% 
  summarise(Intercept = round(coefficients(get(Ajuste))[1], 2),
            Dobles = round(coefficients(get(Ajuste))[2], 2),
            Triples = round(coefficients(get(Ajuste))[3], 2),
            Robos = round(coefficients(get(Ajuste))[4], 2),
            Juegos = round(coefficients(get(Ajuste))[5], 2))

kable(Comparacion, caption = "Tabla N°4: Comparación de los coeficientes estimados por los 3 modelos") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

Al analizar la Tabla N°4, puede verse que los valores óptimos de $\lambda$ varían mucho según se trata de la regresión Ridge o del método de Lasso. Respecto a los coeficientes estimados vemos que, como indica la teoría, en general han reducido su valor. En el caso de la regresión Ridge destaca que se invirtieron los signos del intercepto y la variable juegos. En el caso de Lasso, destaca que el coeficiente de juegos lo forzó a ser igual a 0 y eso trajo aparejado un aumento importante en el valor del intercepto.

## Capacidad predictiva

Una vez presentado el ajuste de los 3 modelos, se evaluó su poder predictivo en base al error cuadrático medio en el conjunto de testeo.

```{r}
testing_X <- testing %>% 
  select(Dobles, Triples, Robos, Juegos) %>% 
  as.matrix()

pred_ridge <- predict(m_ridge, testing_X)
pred_lasso <- predict(m_lasso, testing_X)
pred_MCO <- predict(m1, testing)

MSE_ridge <- mean((testing$Puntos-pred_ridge)^2)
MSE_lasso <- mean((testing$Puntos-pred_lasso)^2)
MSE_MCO <- mean((testing$Puntos-pred_MCO)^2)

modelos$MSE = c(MSE_ridge, MSE_lasso, MSE_MCO)

kable(modelos[,c(1,4)], caption = "Tabla N°5: Comparación de la capacidad predictiva de los 3 modelos") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

Sorprendentemente, la Tabla N°5 muestra que el mejor ajuste fue el de mínimos cuadrados ordinarios, el cual presenta un error 60% menor al de los modelos de Ridge y Lasso. Esto nos indica que, a pesar de que no se cumplan los supuestos y hayamos identificado posibles problemas de colinealidad, a la hora de predecir mínimo cuadrados fue más performante que el resto.

# Regresión Logística

## Creación de nueva variable

Se procede a crear una nueva variable binaria llamada Puntos_200 la cual toma valor 1 si Puntos \>= 200, 0 caso contrario.

```{r}
data <- data %>%
  mutate(
    Puntos_200 = factor(ifelse(Puntos>=200, 1, 0))
    )
```

Con esta nueva variable, se procede a dividir el dataset nuevamente como en el comienzo de este trabajo. Si bien el punto anterior podría haberse hecho en los datasets ya separados, es conveniente hacerlo en el dataset original y generar la partición con la función `createDataPartition` ya que garantiza que la representación de las categorías sea pareja en training y testing.

```{r}
# División en training y testing
set.seed(4567)
#index_training <- sample(1:nrow(data), size = 0.7*nrow(data))
index_training <- createDataPartition(data$Puntos_200, p = 0.7)
training <- data[index_training$Resample1,]
testing <- data[-index_training$Resample1,]
```

## Modelo de Regresión Logística

Creada la variable binaria y dividido el dataset en training y testing, se ajustó el modelo de regresión logística para predecir la probabilidad de que una jugadora anote más de 200 puntos en base a la cantidad de dobles, la cantidad de triples, la cantidad de robos y los partidos jugados.

```{r}
m_logistica <- glm(Puntos_200 ~ Dobles + Triples + Robos + Juegos, family = "binomial", data = training)

```

### Interpretación

```{r}
Betas = data.frame(Variable = names(coef(m_logistica)),
                   Betas = round(as.numeric(coef(m_logistica)), 2),
                   RO = round(exp(as.numeric(coef(m_logistica))), 2),
                   P_value = as.numeric(coef(summary(m_logistica))[, "Pr(>|z|)"]))

kable(Betas, caption = "Tabla N°6: Coeficientes y razones de odds estimados del modelo") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F)
```

A partir de la Tabla N°6 puede verse que solo resultan significativos los efectos de las variables Dobles y Triples. A continuación se presenta su interpretación:

-   Por cada doble extra que anote una jugadora, sus chances de anotar más de 200 puntos en la temporada aumentan un 52% en promedio.

-   Por cada triple extra que anote una jugadora, sus chances de anotar más de 200 puntos en la temporada aumentan un 75% en promedio.

### Punto de corte óptimo y evaluación del modelo

Definido el modelo, se buscó el punto de corte óptimo y se evaluó su performance en el conjunto de testing.

```{r}
curvaROC <- roc(
  response = training$Puntos_200,
  predictor = fitted.values(m_logistica),
  quiet = TRUE
)

plot(curvaROC, print.auc = TRUE)

pdc = pROC::coords(curvaROC, "best", ret = "threshold")

```

```{r}
testing$Predichos <- factor(ifelse(predict(m_logistica, newdata = testing, type = "response") >= pdc$threshold, 1, 0))

library(janitor)

tabyl(dat = testing, Puntos_200, Predichos) %>% 
  adorn_totals(where = c("row", "col"))

metricas = confusionMatrix(data = testing$Predichos, reference = testing$Puntos_200, positive = "1", mode = "everything",)

medidas = data.frame(Precisión = round(as.numeric(metricas$overall[1]), 3),
                     Sensibilidad = round(as.numeric(metricas$byClass[1]), 3),
                     Especificidad = round(as.numeric(metricas$byClass[2]), 3),
                     VPP = round(as.numeric(metricas$byClass[3]), 3),
                     VPN = round(as.numeric(metricas$byClass[4]), 3),
                     Kappa = round(as.numeric(metricas$overall[2]), 3),
                     "F1-Score" = round(as.numeric(metricas$byClass[7]), 3))

kable(medidas, caption = paste0("Tabla N°7: Medidas de ajuste del modelo con un punto de corte ", round(pdc$threshold, 3))) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F)
```

A continuación se interpretan las medidas presentadas en la Tabla N°7:

-   **Precisión:** El modelo predice correctamente al 96,3% de las jugadoras.

-   **Sensibilidad:** El modelo predice correctamente al 95,7% de las jugadoras que anotaron más de 200 puntos en la temporada.

-   **Especificidad:** El modelo predice correctamente al 96,8% de las jugadoras que anotaron menos de 200 puntos en la temporada.

-   **VPP:** El 95,7% de las jugadoras que el modelo predice que anotaron más de 200 puntos efectivamente lo hicieron en la realidad.

-   **VPN:** El 96,8% de las jugadoras que el modelo predice que no anotaron más de 200 puntos efectivamente no lo hicieron en la realidad.

-   **Kappa:** Dado que el coeficiente kappa dio cercano a 1, se puede decir que el ajuste del modelo es muy bueno.

-   **F1-Score:** Dado que la sensibilidad y el VPP nos dieron iguales, el valor de F1 score también coincide. Nuevamente, este valor indica un muy buen ajuste del modelo.
