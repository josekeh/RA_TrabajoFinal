---
title: "Trabajo Práctico Final"
format: pdf
lang: es 
editor: visual
editor_options: 
  chunk_output_type: console
execute: 
  echo: false
  warning: false
  error: true
---

# Introducción

El básquetbol es uno de los deportes más populares en los Estados Unidos. Además de su destacadísima liga masculina NBA, se encuentra su similar femenina, la altamente competitiva, WNBA. El siguiente trabajo busca demostrar conocimientos parendidos en la materia Regresión avanzada a partir de un dataset de esta ultima liga mencionada.

# Dataset

## Librerías

```{r}
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(broom)
library(egg)
library(ggpubr)
library(car)
library(glmnet)
```

## Importación de datos

```{r}
data <- read.csv("files/WNBA.csv", sep = ";")
summary(data)
```

```{r}
data$Equipo <- as.factor(data$Equipo)
data$Posicion <- as.factor(data$Posicion)
summary(data)
```

## Breve análisis

### Vacíos y duplicados

A través del summary se puede ver que las variables numéricas están correctamente definidas, sin NAs. Los mismo para Equipo y Posición. Se procede a analizar si existen jugadoras campo Jugadora vacío

```{r}
paste0("Hay ",count(data %>% filter(Jugadora == "")), " registros vacíos")
```

Una vez checkeado esto, viendo que no hay vacíos, vemos si hay alguna jugadora repetida.

```{r}
data[duplicated(data$Jugadora),]
```

Vemos que existen jugadoras que aparecen más de una vez, pero parece ser que se encuentran en equipos diferentes.

```{r}
print(count(data[duplicated(data[, c("Jugadora", "Equipo")]), ]))
```

Por ende, podemos entender que cada vez que aparece una jugadora se da en otro club. Esto también puede llevar a diferentes condiciones de goleo como así también otro momento de la carrera profesional, por lo que se decide conservar los datos tal cual se encuentran.

### Entendiendo el dataset

Una vez avanzada la etapa anterior, vamos a ver brevemente cómo se encuentran distribuidos los datos.

```{r}
ggplot(data, aes(x = Equipo)) +
  geom_bar(fill = "grey", color = "black") +
  labs(title = "Equipos",
       x = "Equipo",
       y = "Frecuencia") +
  theme_minimal()
```

```{r}
ggplot(data, aes(x = Posicion)) +
  geom_bar(fill = "grey", color = "black") +
  labs(title = "Equipos",
       x = "Posición",
       y = "Frecuencia") +
  theme_minimal()
```

Se puede observar como es similar la distribución de jugadoras por equipo, mientras que abundan mayormente posiciones de Escolta, seguido de Alero y luego Pivot. El tener pocos datos de Pivots no es algo irracional, ya que es es la posición que menos jugadoras suele haber.

```{r}
skimr::skim(data %>% select(where(is.numeric)))
```

Analizando las variables cuantitativas vemos en primer lugar que no hay valores negativos, lo cual es coherente y requerido. Mientras que la mayoría de las distribuciones son asimétricas hacia la derecha, se puede observar como la cantidad de juegos es la única que no respeta este patrón. Resulta apropiado ver cómo el histograma de dobles y triples es muy parecido en la forma de la distribución como así también el resultante natural de los puntos.

También es interesante comparar medias de Triples y Dobles con Puntos. Si se realizar rápidamente Triples x 3 + Dobles x 2 (en medias), se obtiene un valor de 186.7, lejano al 226 de los Puntos, por lo que podríamos entender, a priori, que los libres (tiros individuales de 1 punto) tiene peso en el goleo de las jugadoras.

# Modelos de Regresión Lineal

Para comenzar a trabajar en los modelos de regresión lineal, se procede a dividir el dataset en train/test. Se define una semilla para hacer reproducible el experimento y se define, por consigna, que el train contenga el 70% de los datos, mientras que lo restante queda para test.

```{r}
# División en training y testing
set.seed(45672848)
index_training <- sample(1:nrow(data), size = 0.7*nrow(data))

training <- data[index_training,]
testing <- data[-index_training,]
```

Se procede, luego, a obtener tres modelos mediante los siguientes criterios: - Usando stepwise regression - Criterios propios - Preguntando al famoso ChatGPT

## Stepwise regression

Para el primer caso se busca obtener un modelo de regresión en el que las variables a utilizar se definan por el proceso automático de Stepwise regression, ya que utilizar la técnica del mejor subconjunto llevaría a un número muy grande de modelos a comprar y se opta por esta técnica que si bien no garantiza encontrar el óptimo, puede darnos una buena primera aproximación. Para el análisis se quitan del dataset el nombre de la jugadora y el equipo, ya que dichas variables cuentan con una gran cantidad de categorías. En caso de incluirlas, se obtendrían estimaciones muy pobres debido a los escasos grados de libertad, o incluso podría llegar el caso que sea imposible estimar el modelo.

```{r}
training_filtered <- select(training, -Jugadora, -Equipo)
m1 <- MASS::stepAIC(
  object = lm(Puntos ~ 1, data = training_filtered), #punto de partida
  scope = list(upper = lm(Puntos ~ ., data = training_filtered)), #máximo modelo posible
  direction = "both", #método de selección
  trace = FALSE, #para no imprimir resultados parciales
  k = 2, #penalización a emplear (2 = AIC, log(n) = BIC)
  steps = 1000 #máximo nro de pasos
  )

summary(m1)
```

El resultado devuelve la utilización de Dobles, Triples, Robos y Juegos.

## Criterios propios

Observando el dataset, se plantea un modelo donde los puntos se basen en dobles y triples, y debido a que debería de haber una diferencia (por los libres), se plantea la posición, la cual determinaría la tendencia de un jugador a recibir faltas de tiro.

```{r}
m2 <- lm(Puntos ~ Dobles + Triples + Posicion, data = training_filtered)

summary(m2)
```

## ChatGPT

Para este tercer modelo, se le ha preguntado (o desafiado) a ChatGPT a recomendar cuál sería el mejor modelo. La respuesta fue incluir Dobles, Triples, Minutos, Asistencias, Rebotes y Posicion.

```{r}
m3 <- lm(Puntos ~ Triples + Dobles + Minutos + Asistencias + Rebotes + Posicion, data = training)
```

## Comparación de modelos

Se procede a comparar los modelos planteados comparando CME, PRESS, Cp, AIC y BIC.

```{r}
modelos <- data.frame(Modelo = c("Modelo 1", "Modelo 2", "Modelo 3"),
                      Ajuste = c("m1", "m2", "m3"))
```

```{r}
Modelo_completo <- lm(Puntos ~ . -Jugadora - Equipo, data = training)
sigma <- sum(residuals(Modelo_completo)^2)/df.residual(Modelo_completo)

Resumen <- modelos %>% 
  group_by(Modelo) %>% 
  summarise(CME = summary(get(Ajuste))$sigma^2,
            PRESS = sum((resid(get(Ajuste))/(1-hatvalues(get(Ajuste))))^2),
            Cp = sum(residuals(get(Ajuste))^2)/sigma + 2 * length(coef(get(Ajuste))) - nrow(training),
            AIC = AIC(get(Ajuste)),
            BIC = BIC(get(Ajuste)))

kable(Resumen, caption = "Tabla N°1: Comparación del ajuste de los 3 modelos") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

Con los datos obtenidos se puede pensar en elegir entre el Modelo 1 y Modelo 2. El Modelo 1 es el que mejores resultados en las métricas de performance, mientras que el Modelo 2 es apenas peor, pero es más simple. Para este caso, se decide quedarse con el Modelo 1.

## Análisis de residuos

Ya con el Modelo 1 elegido, se procede a evaluar los supuestos a través de los residuos.

```{r}
Modelo_final <- augment(m1)
g1 <- ggplot(mapping = aes(sample = Modelo_final$.resid)) + 
    stat_qq(color = "dodgerblue") + 
    stat_qq_line() + 
  ggtitle("QQ Normal") +
  xlab("Cuantiles teóricos") +
  ylab("Cuantiles muestrales") +
  geom_text(aes(x = 2, y = -50, fill ="black", label = paste0("p-value \n", round(shapiro.test(Modelo_final$.resid)$p.value, 8)))) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = "none")

g2 <- ggplot(Modelo_final) +
  aes(x = .fitted, y = .std.resid) +
  geom_point(color = "dodgerblue") +
  geom_hline(yintercept = 0, color = "black") +
  ggtitle("Residuos vs. Predichos") +
  xlab("Valores predichos") +
  ylab("Residuos estandarizados") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

g <- ggarrange(g1, g2, ncol = 2)

annotate_figure(g, top = text_grob("Gráfico N°1: Comprobación de supuestos", face = "bold", size = 14))
```

En el gráfico N°1 se observa que la distribución de los mismos presenta colas pesadas y que no se verifica el supuesto de variancia constante. En caso de querer solucionarlo, podría optarse por utilizar una transformación de la variables respuesta como el $log(Y)$ o bien aplicar el método de Box y Cox.

Luego se procede a analizar colinealidad

```{r}
# Colinealidad
training_modelo <- training %>% 
  select(Juegos, Minutos, Triples, Dobles)

corr_matrix <- cor(training_modelo)

corr_melted <- reshape2::melt(corr_matrix)

ggplot(corr_melted, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  geom_text(aes(label = round(value, 2)), size = 3) +
  scale_fill_gradient2(low = "#ef2947", high = "#009929", mid = "white", midpoint = 0) +
  labs(x = "", y = "", title = "Gráfico N°2: Matriz de correlación") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```

Analizando las correlaciones lineales entre variables se observa que minutos está muy relacionada linealmente con todas las restantes.

```{r}
vif = vif(m1)

vif_df <- data.frame(Variable = names(vif),
                     VIF = as.numeric(vif),
                     row.names = NULL)

kable(vif_df, caption = "Tabla N°2: Factores de Inflación de la Varianza (VIF)") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F)
```

Al analizar la tabla N°2, se ve que, a pesar de que las variables estén altamente correlacionadas, esto no tiene un impacto tan importante ya que ningún VIF es mayor a 5.

```{r}
# Observaciones atípicas
Modelo_final <- mutate(Modelo_final, 
                       PRESS = .resid/(1-.hat),
                       id = 1:nrow(Modelo_final))

g1 <- ggplot(data = Modelo_final) + 
    aes(x = id, y = PRESS) + 
  geom_bar(stat="identity", color = "dodgerblue", fill = "dodgerblue")+
    geom_hline(aes(yintercept = 0)) +
    ggtitle("Residuos PRESS") +
    xlab("Observación") +
    ylab("PRESS") +
    theme_bw() +
    theme(legend.position = "none",
          plot.title = element_text(hjust = 0.5))

# Observaciones influyentes
g2 <- ggplot(data = Modelo_final) +
    aes(x = id, y = .cooksd) +
  geom_bar(stat="identity", color = "dodgerblue", fill = "dodgerblue")+
    ggtitle("Distancia de Cook") +
    xlab("Observación") +
    ylab("Distancia de Cook") +
    theme_bw() +
    theme(legend.position = "none",
      plot.title = element_text(hjust = 0.5))

g <- ggarrange(g1, g2, ncol = 2)

annotate_figure(g, top = text_grob("Gráfico N°3: Observaciones atípicas e influyentes", face = "bold", size = 14))
```

En el gráfico N°3 se vuelve a ver que existe una observación atípica, la cual presenta un residuo PRESS mayor a 100. Sin embargo, al analizar la influencia de las observaciones mediante la distancia de Cook, se encuentra que ninguna tiene una influencia desmedida sobre el ajuste del modelo, ya que todas son menores a 1.

## Interpretación del modelo

```{r}
Betas = data.frame(Variable = names(coef(m1)),
                   Valor = as.numeric(coef(m1)),
                   P_value = as.numeric(coef(summary(m1))[, "Pr(>|t|)"]))

kable(Betas, caption = "Tabla N°3: Coeficientes estimados del modelo") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F)

as.numeric(coef(summary(m2))[, "Pr(>|t|)"])
```

A través de la Tabla N°3 se puede interpretar sobre nuestro modelo elegido:

-   El valor estimado del intercepto es de 2.68 aproximadamente. Igual así, el pvalue resulta de no poder descartar la hipótesis de que el intercepto es igual a 0. Recordando la interpretación del mismo, se pensaría que es el valor de puntos promedio para 0 dobles, 0 triples, 0 robos y 0 juegos. Lo cual no resulta para nada ilógico pensar que este valor sea 0 en la realidad y que estadísticamente nos de dicho indicio.

Los demás predictores tienen un pvalue <0.01, algunos con ordenes mucho menores, por lo que podemos rechazar la hipotesis de nulidad con bastante confianza. Respecto a sus valores predichos podemos decir que:

-   Por cada doble convertido, los puntos promedio de cada jugadora aumentan en una suma de 2.40, manteniendose todas las demás variables constantes.
-   Por cada triple convertido, los puntos promedio de cada jugadora aumentan en una suma de 3.46, manteniendose todas las demás variables constantes.
-   Por cada robo conseguido, los puntos promedio de cada jugadora aumentan en una suma de 0.74, manteniendose todas las demás variables constantes.
-   Por cada juego jugado, los puntos promedio de cada jugadora disminuyen en una suma de 0.51, manteniendose todas las demás variables constantes.

Estas interpretaciones nos dejan insights interesantes de analizar. Si bien dobles y triples suman 2 y 3 puntos respectivamente en el juego, que el valor que afectan al promedio sea mayor puede estar indicando que en cierto modo, se absorbe esa probabilidad de libre comentada anteriormente. Lo mismo podemos pensar acerca del robo, ya que cuando se da el mismo, en el desorden del juego, es potable a que quien roba reciba una falta. Por último, el valor al aumentar la cantidad de juegos podría estar dando indicio de desgaste físico de las jugadoras.

# Métodos de Regularización

Una vez identificado el mejor modelo de regresión con el ajuste de mínimos cuadrados ordinarios, se optó por evaluar la posibilidad de utilizar métodos de regularización. Esto se hizo con el objetivo de intentar mejorar la capacidad predictiva, basándonos en la idea del *trade-off* entre sesgo y variancia.

Para realizar tanto el ajuste mediante la regresión ridge, como por el método de lasso, primero se buscó el mejor valor de $\lambda$ utilizando *5-fold Cross-Validation.*

Los resultados de los ajustes junto con los valores de $\lambda$ utilizados, son presentados a continuación:

```{r}
# Preparamos la matriz de predictores
training_X <- training %>% 
  select(Dobles, Triples, Robos, Juegos) %>% 
  as.matrix()

# Buscamos el lambda óptimo para ridge y lasso
lambda_ridge <- cv.glmnet(training_X, training$Puntos, nfolds = 5, alpha = 0)$lambda.min 

lambda_lasso <- cv.glmnet(training_X, training$Puntos, nfolds = 5, alpha = 1)$lambda.min 

# Ajustamos la regresión ridge y la regresión lasso
m_ridge <- glmnet(x = training_X, y = training$Puntos, alpha = 0, lambda = lambda_ridge)

m_lasso <- glmnet(x = training_X, y = training$Puntos, alpha = 1, lambda = lambda_ridge)
coefficients(m_lasso)
```

```{r}
modelos <- data.frame(Modelo = c("Modelo Ridge", "Modelo Lasso", "Modelo MCO"),
                      Lambda = c(round(lambda_ridge, 3), round(lambda_lasso, 3), "-"),
                      Ajuste = c("m_ridge", "m_lasso", "m1"))

modelos$Modelo <- factor(modelos$Modelo, levels = c("Modelo Ridge", "Modelo Lasso", "Modelo MCO"))

Comparacion <- modelos %>% 
  group_by(Modelo, Lambda) %>% 
  summarise(Intercept = round(coefficients(get(Ajuste))[1], 2),
            Dobles = round(coefficients(get(Ajuste))[2], 2),
            Triples = round(coefficients(get(Ajuste))[3], 2),
            Robos = round(coefficients(get(Ajuste))[4], 2),
            Juegos = round(coefficients(get(Ajuste))[5], 2))

kable(Comparacion, caption = "Tabla N°4: Comparación de los coeficientes estimados por los 3 modelos") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

Al analizar la Tabla N°4, puede verse que los valores óptimos de $\lambda$ varían mucho según se trata de la regresión Ridge o del método de Lasso. Respecto a los coeficientes estimados vemos que, como indica la teoría, en general han reducido su valor. En el caso de la regresión Ridge destaca que se invirtieron los signos del intercepto y la variable juegos. En el caso de Lasso, destaca que el coeficiente de juegos lo forzó a ser igual a 0 y eso trajo aparejado un aumento importante en el valor del intercepto.

## Capacidad predictiva

Una vez presentado el ajuste de los 3 modelos, se evaluó su poder predictivo en base al error cuadrático medio en el conjunto de testeo.

```{r}
testing_X <- testing %>% 
  select(Dobles, Triples, Robos, Juegos) %>% 
  as.matrix()

pred_ridge <- predict(m_ridge, testing_X)
pred_lasso <- predict(m_lasso, testing_X)
pred_MCO <- predict(m1, testing)

MSE_ridge <- mean((testing$Puntos-pred_ridge)^2)
MSE_lasso <- mean((testing$Puntos-pred_lasso)^2)
MSE_MCO <- mean((testing$Puntos-pred_MCO)^2)

modelos$MSE = c(MSE_ridge, MSE_lasso, MSE_MCO)

kable(modelos[,c(1,4)], caption = "Tabla N°5: Comparación de la capacidad predictiva de los 3 modelos") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

Sorprendentemente, la Tabla N°5 muestra que el mejor ajuste fue el de mínimos cuadrados ordinarios, el cual presenta un error 60% menor al de los modelos de Ridge y Lasso. Esto nos indica que, a pesar de que no se cumplan los supuestos y hayamos identificado posibles problemas de colinealidad, a la hora de predecir mínimo cuadrados fue más performante que el resto.

# Regresión Logística

## Creación de nueva variable

Se procede a crear una nueva variable bienaria llamada Puntos_200 la cual toma valor 1 si Puntos \>= 200, 0 caso contrario.

```{r}
data <- data %>%
  mutate(
    Puntos_200 = ifelse(Puntos>=200, 1, 0)
    )
```

Con esta nueva variable, se procede a dividir el dataset nuevamente como en el comienzo de este trabajo. Está claro que el punto anterior podría haberse hecho en los datasets ya separados, pero como lo indicaba la consigna, la variable se creó en el dataset original.

```{r}
# División en training y testing
set.seed(45672848)
index_training <- sample(1:nrow(data), size = 0.7*nrow(data))

training <- data[index_training,]
testing <- data[-index_training,]
```
